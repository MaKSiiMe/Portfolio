# ğŸ“˜ Glossaire et Concepts ClÃ©s - Reinforcement Learning pour Jeux de Cartes

## Table des MatiÃ¨res
1. [ğŸ§  Concepts Fondamentaux](#-concepts-fondamentaux)
2. [ğŸ“¦ Algorithmes de RL](#-algorithmes-de-rl)
3. [ğŸ— Architectures de RÃ©seaux](#-architectures-de-rÃ©seaux)
4. [ğŸ”§ AutoML / AutoRL et Optimisation](#-automl--autorrl-et-optimisation)
5. [âš™ï¸ Environnements et Outils](#ï¸-environnements-et-outils)
6. [ğŸ“– DÃ©finitions plus dÃ©taillÃ©es](#-dÃ©finitions-plus-dÃ©taillÃ©es)

---

## ğŸ§  Concepts Fondamentaux

| Terme                          | Description                                                                 |
|--------------------------------|-----------------------------------------------------------------------------|
| **[MDP (Markov Decision Process)](#01-mdp-markov-decision-process)** | ModÃ©lisation mathÃ©matique dâ€™un environnement de dÃ©cision avec Ã©tats, actions, transitions, rÃ©compenses.  |
| **[Reward shaping](#02-reward-shaping)**                             | Modification de la fonction de rÃ©compense pour guider lâ€™apprentissage plus efficacement.                 |
| **[Curriculum learning](#03-curriculum-learning)**                   | Apprentissage progressif, dÃ©butant par des tÃ¢ches faciles.                                               |
| **[Self-play](#04-self-play)**                                       | Lâ€™agent joue contre lui-mÃªme ou ses versions prÃ©cÃ©dentes pour apprendre.                                 |
| **[Policy](#05-policy)**                                             | Fonction dÃ©terminant lâ€™action Ã  prendre selon lâ€™Ã©tat. Peut Ãªtre dÃ©terministe ou stochastique.            |
| **[Value Function](#06-value-function)**                             | Fonction estimant la rÃ©compense attendue dâ€™un Ã©tat ou dâ€™une action donnÃ©e.                               |
| **[Exploration vs Exploitation](#07-exploration-vs-exploitation)**   | Dilemme entre exploiter les connaissances acquises ou explorer de nouvelles stratÃ©gies.                  |
| **[Hyperparameters](#08-hyperparameters)**                           | ParamÃ¨tres dÃ©finis avant lâ€™entraÃ®nement (learning rate, gamma, epsilon, etc.).                           |
| **[Discount Factor](#09-discount-factor)**                           | ContrÃ´le lâ€™importance des rÃ©compenses futures.                                                           |
| **[Episode](#10-episode)**                                           | Une sÃ©quence complÃ¨te dâ€™interactions entre lâ€™agent et lâ€™environnement.                                   |
| **[State Space](#11-state-space)**                                   | Ensemble de tous les Ã©tats possibles de lâ€™environnement.                                                 |
| **[Action Space](#12-action-space)**                                 | Ensemble de toutes les actions possibles dans un Ã©tat donnÃ©.                                             |


---

## ğŸ“¦ Algorithmes de RL

| Algorithme                     | Description                                                                 |
|--------------------------------|-----------------------------------------------------------------------------|
| **[DQN (Deep Q-Network)](#05-dqn-deep-q-network)**                     | RÃ©seau de neurones apprenant une fonction Q(s, a) pour des environnements discrets. |
| **[PPO (Proximal Policy Optimization)](#06-ppo-proximal-policy-optimization)** | Algorithme acteur-critique stable et performant. |
| **[A2C / A3C](#07-a2c--a3c-advantage-actor-critic--asynchronous-a2c)** | Versions synchrones/asynchrones de lâ€™acteur-critique. |
| **[MCTS (Monte Carlo Tree Search)](#08-mcts-monte-carlo-tree-search)**  | Recherche sÃ©quentielle pour explorer les meilleures actions Ã  prendre. |
| **[AlphaZero / MuZero](#09-alphazero--muzero)**                         | Algos combinant MCTS, RL profond et self-play (MuZero apprend mÃªme les rÃ¨gles). |

---

## ğŸ— Architectures de RÃ©seaux

| Architecture                   | Description                                                                 |
|--------------------------------|-----------------------------------------------------------------------------|
| **[MLP (Multi-Layer Perceptron)](#10-mlp-multi-layer-perceptron)**       | RÃ©seau de neurones dense classique. |
| **[LSTM (Long Short-Term Memory)](#11-lstm-long-short-term-memory)**     | RÃ©seau rÃ©current pour sÃ©quences (ex : ordre de draft). |
| **[CNN (Convolutional Neural Network)](#12-cnn-convolutional-neural-network)** | Peut extraire des patterns sur des sÃ©quences de cartes. |
| **[Transformer](#13-transformer)**                                      | RÃ©seau Ã  attention, puissant sur sÃ©quences longues. |
| **[GNN (Graph Neural Network)](#14-gnn-graph-neural-network)**          | Encode des relations structurelles (ex : cartes interconnectÃ©es). |

---

## ğŸ”§ AutoML / AutoRL et Optimisation

| Terme / Outil                  | Description                                                                 |
|--------------------------------|-----------------------------------------------------------------------------|
| **[AutoML](#15-automl-automated-machine-learning)**                     | Automatisation du choix de modÃ¨le, tuning et architecture. |
| **[AutoRL](#16-autorrl-automated-reinforcement-learning)**              | Application dâ€™AutoML au RL (paramÃ¨tres, rewards, architectures). |
| **[NAS (Neural Architecture Search)](#17-nas-neural-architecture-search)** | Recherche automatique des meilleures architectures de rÃ©seau. |
| **[Ray Tune](#18-ray-tune)**                                           | Librairie de tuning dâ€™hyperparamÃ¨tres Ã  grande Ã©chelle. |
| **[Optuna / HyperOpt / Nevergrad](#19-optuna--hyperopt--nevergrad)**    | Librairies pour optimiser automatiquement les hyperparamÃ¨tres. |

---

## âš™ï¸ Environnements et Outils

| Outil                           | Description                                                                 |
|---------------------------------|-----------------------------------------------------------------------------|
| **[OpenAI Gym / Gymnasium](#20-openai-gym--gymnasium)**                  | Standard pour implÃ©menter des environnements RL. |
| **[RLCard](#21-rlcard)**                                               | Environnements RL pour jeux de cartes (UNO, Blackjack, etc.). |
| **[PettingZoo](#22-pettingzoo)**                                        | Environnements multi-agents (ex : 2 joueurs). |
| **[Stable-Baselines3 (SB3)](#23-stable-baselines3-sb3)**                | ImplÃ©mentations prÃªtes Ã  lâ€™emploi des algos de RL. |
| **[RLlib](#24-rllib-ray)**                                             | Librairie distribuÃ©e pour entraÃ®nement RL Ã  grande Ã©chelle. |

---

## ğŸ“– DÃ©finitions plus dÃ©taillÃ©es

### 01. MDP (Markov Decision Process)
<details>
<summary>Voir la dÃ©finition</summary>

Un MDP est un cadre mathÃ©matique qui dÃ©crit comment un agent interagit avec un environnement Ã  travers:
- **S (States)** : les Ã©tats du jeu (ex : contenu du deck, main actuelle, PV des PokÃ©mon actifs)
- **A (Actions)** : les actions disponibles (ex : choisir une carte Ã  ajouter au deck, lancer une attaque)
- **P (Transitions)** : les probabilitÃ©s de passer dâ€™un Ã©tat Ã  un autre aprÃ¨s une action
- **R (Reward)** : le score ou feedback reÃ§u
- **Î³ (Gamma)** : un facteur de pondÃ©ration pour les rÃ©compenses futures

ğŸ§ª **Application Ã  ton projet** :
- Ã‰tat = composition actuelle du deck + rÃ©sumÃ© des performances passÃ©es
- Action = ajouter/retirer une carte du deck
- RÃ©compense = victoire/dÃ©faite ou score de performance du deck en match simulÃ©

ğŸ§  **Avantage** : Fournit une base formelle claire pour modÃ©liser ton problÃ¨me dâ€™optimisation de deck comme un processus dâ€™apprentissage par renforcement.

[â¬† Retour Ã  la table des matiÃ¨res](#table-des-matiÃ¨res)

</details>

---

### 02. Reward Shaping
<details>
<summary>Voir la dÃ©finition</summary>

Câ€™est lâ€™art dâ€™ajuster la fonction de rÃ©compense pour faciliter lâ€™apprentissage. Au lieu de ne rÃ©compenser quâ€™Ã  la victoire, on donne aussi des rÃ©compenses intermÃ©diaires.

ğŸ§ª **Application Ã  ton projet** :
- Donner une petite rÃ©compense si un deck fait plus de dÃ©gÃ¢ts moyens par partie
- Donner un bonus si une carte ajoutÃ©e augmente le taux de victoire
- Donner une pÃ©nalitÃ© si le deck dÃ©passe la limite autorisÃ©e de doublons ou devient moins polyvalent

ğŸ§  **Avantage** : AccÃ©lÃ¨re lâ€™apprentissage et Ã©vite que lâ€™IA stagne ou apprenne des comportements sous-optimaux.

[â¬† Retour Ã  la table des matiÃ¨res](#table-des-matiÃ¨res)

</details>

---

### 03. Curriculum Learning
<details>
<summary>Voir la dÃ©finition</summary>

On commence lâ€™entraÃ®nement sur des tÃ¢ches simples, puis on augmente la difficultÃ© progressivement, comme dans un programme scolaire.

ğŸ§ª **Application Ã  ton projet** :
- Phase 1 : Lâ€™IA optimise un deck avec 5 cartes fixes et ne peut changer quâ€™1 ou 2 cartes
- Phase 2 : Lâ€™IA a un choix libre sur 15 cartes parmi un pool limitÃ©
- Phase 3 : Lâ€™IA peut construire un deck complet avec toute la base de donnÃ©es

ğŸ§  **Avantage** : Rend lâ€™apprentissage plus stable et progressif, surtout dans des environnements complexes Ã  forte combinatoire comme PokÃ©mon TCG Pocket.

[â¬† Retour Ã  la table des matiÃ¨res](#table-des-matiÃ¨res)


</details>

---

### 04. Self-play
<details>
<summary>Voir la dÃ©finition</summary>

Lâ€™agent joue contre lui-mÃªme ou ses versions prÃ©cÃ©dentes, ce qui permet dâ€™apprendre sans supervision externe.

ğŸ§ª **Application Ã  ton projet** :
- Lâ€™IA construit un deck, puis joue des parties contre un adversaire IA entraÃ®nÃ© prÃ©cÃ©demment
- Tu peux faire Ã©voluer ce second adversaire au fil du temps, pour forcer lâ€™IA Ã  sâ€™adapter Ã  des decks toujours plus performants

ğŸ§  **Avantage** : Permet un entraÃ®nement autonome et indÃ©fini, en gÃ©nÃ©rant des adversaires dynamiques adaptÃ©s au niveau de lâ€™agent.

[â¬† Retour Ã  la table des matiÃ¨res](#table-des-matiÃ¨res)


</details>

---

### 05. DQN (Deep Q-Network)
<details>
<summary>Voir la dÃ©finition</summary>

Un algorithme qui apprend Ã  estimer la valeur Q(s, a), câ€™est-Ã -dire la qualitÃ© dâ€™une action a dans un Ã©tat s. Il utilise un rÃ©seau de neurones pour approximer cette fonction.

ğŸ§ª **Application Ã  ton projet** :
- Lâ€™Ã©tat pourrait Ãªtre la composition actuelle du deck
- Lâ€™action serait ajouter, retirer ou remplacer une carte
- Le rÃ©seau apprend quelles actions mÃ¨nent aux meilleurs taux de victoire

ğŸ§  **Avantage** : bien adaptÃ© si tu as un espace dâ€™action discret (ex : pool limitÃ© de cartes).

[â¬† Retour Ã  la table des matiÃ¨res](#table-des-matiÃ¨res)


</details>

---

### 06. PPO (Proximal Policy Optimization)
<details>
<summary>Voir la dÃ©finition</summary>

Un algorithme acteur-critique qui apprend directement une politique (Ï€) pour choisir des actions, tout en restant proche de la politique prÃ©cÃ©dente (dâ€™oÃ¹ "proximal").

ğŸ§ª **Application Ã  ton projet** :
- Ton IA peut apprendre une distribution de choix de cartes, au lieu de choisir toujours la mÃªme
- Elle sâ€™ajuste progressivement pour Ã©viter les comportements instables

ğŸ§  **Avantage** : TrÃ¨s utilisÃ©, stable, et compatible avec des architectures plus complexes comme des [Transformers](#13-transformer) ou [GNNs](#14-gnn-graph-neural-network). Utile pour entraÃ®ner des politiques stochastiques.

[â¬† Retour Ã  la table des matiÃ¨res](#table-des-matiÃ¨res)


</details>

---

### 07. A2C / A3C (Advantage Actor-Critic / Asynchronous A2C)
<details>
<summary>Voir la dÃ©finition</summary>

Algorithmes avec deux rÃ©seaux :
- Acteur : choisit lâ€™action
- Critique : estime la valeur de lâ€™Ã©tat
La version A3C entraÃ®ne plusieurs agents en parallÃ¨le.

ğŸ§ª **Application Ã  ton projet** :
- Tu peux entraÃ®ner plusieurs agents avec des decks diffÃ©rents en parallÃ¨le et agrÃ©ger leur apprentissage
- Parfait pour accÃ©lÃ©rer lâ€™entraÃ®nement via [self-play](#04-self-play) en parallÃ¨le

ğŸ§  **Avantage** : TrÃ¨s rapide Ã  entraÃ®ner sur CPU et efficace pour les tÃ¢ches complexes avec beaucoup de bruit dans la rÃ©compense (comme des rÃ©sultats de match).

[â¬† Retour Ã  la table des matiÃ¨res](#table-des-matiÃ¨res)


</details>

---

### 08. MCTS (Monte Carlo Tree Search)
<details>
<summary>Voir la dÃ©finition</summary>

Une mÃ©thode basÃ©e sur la simulation qui construit un arbre de dÃ©cisions en explorant les branches les plus prometteuses.
Elle simule des jeux jusquâ€™au bout pour estimer la valeur dâ€™une action.

ğŸ§ª **Application Ã  ton projet** :
- UtilisÃ© pour tester diffÃ©rents choix de decks, en simulant des parties pour chaque branche
- Peut aider Ã  sÃ©lectionner les meilleures actions de construction ou mÃªme les meilleures stratÃ©gies de jeu

ğŸ§  **Avantage** : TrÃ¨s utile au dÃ©but du projet, quand tu nâ€™as pas encore de modÃ¨le appris. Peut aussi Ãªtre combinÃ© avec un rÃ©seau de valeur (comme dans [AlphaZero)](#09-alphazero--muzero)).

[â¬† Retour Ã  la table des matiÃ¨res](#table-des-matiÃ¨res)


</details>

---

### 09. AlphaZero / MuZero
<details>
<summary>Voir la dÃ©finition</summary>

AlphaZero combine [self-play](#04-self-play), [MCTS](#08-mcts-monte-carlo-tree-search) et deep learning.
MuZero va plus loin : il nâ€™a pas besoin de connaÃ®tre les rÃ¨gles du jeu Ã  lâ€™avance. Il apprend un modÃ¨le interne de lâ€™environnement.

ğŸ§ª **Application Ã  ton projet** :
- Lâ€™IA apprend Ã  construire un deck sans savoir explicitement pourquoi certaines cartes sont fortes
- Elle sâ€™appuie uniquement sur les rÃ©sultats de simulation et apprend les "rÃ¨gles" de succÃ¨s implicitement

ğŸ§  **Avantage** : TrÃ¨s puissant dans des environnements complexes ou imparfaitement connus (comme PokÃ©mon TCG Pocket oÃ¹ les synergies entre cartes ne sont pas toujours Ã©videntes).

[â¬† Retour Ã  la table des matiÃ¨res](#table-des-matiÃ¨res)


</details>

---

### 10. MLP (Multi-Layer Perceptron)
<details>
<summary>Voir la dÃ©finition</summary>

Un rÃ©seau dense classique, composÃ© de plusieurs couches entiÃ¨rement connectÃ©es (fully connected). Câ€™est la base des rÃ©seaux neuronaux.

ğŸ§ª **Application Ã  ton projet** :
- EntrÃ©e : reprÃ©sentation vectorielle du deck (par exemple, un vecteur binaire indiquant quelles cartes sont prÃ©sentes)
- UtilisÃ© pour prÃ©dire la valeur du deck (ex : taux de victoire estimÃ©)

ğŸ§  **Avantage** : Simple, rapide Ã  entraÃ®ner, suffisant si tes donnÃ©es sont bien structurÃ©es et peu sÃ©quentielles.

[â¬† Retour Ã  la table des matiÃ¨res](#table-des-matiÃ¨res)


</details>

---

### 11. LSTM (Long Short-Term Memory)
<details>
<summary>Voir la dÃ©finition</summary>

Un type de rÃ©seau rÃ©current (RNN) conÃ§u pour mÃ©moriser les sÃ©quences avec dÃ©pendances Ã  long terme.

ğŸ§ª **Application Ã  ton projet** :
- Peut Ãªtre utilisÃ© si tu veux apprendre Ã  construire un deck carte aprÃ¨s carte, en tenant compte de lâ€™ordre
- Utile pour prÃ©dire la synergie dâ€™une carte ajoutÃ©e en fonction des cartes prÃ©cÃ©demment sÃ©lectionnÃ©es

ğŸ§  **Avantage** : GÃ¨re trÃ¨s bien les donnÃ©es sÃ©quentielles, par exemple pour apprendre une logique de draft ou de combo.

[â¬† Retour Ã  la table des matiÃ¨res](#table-des-matiÃ¨res)


</details>

---

### 12. CNN (Convolutional Neural Network)
<details>
<summary>Voir la dÃ©finition</summary>

Initialement conÃ§u pour les images, mais utilisable pour capturer des motifs locaux dans des sÃ©quences, grÃ¢ce aux filtres convolutifs.

ğŸ§ª **Application Ã  ton projet** :
- Peut dÃ©tecter des motifs ou combinaisons frÃ©quentes de cartes dans les decks gagnants
- EntrÃ©e : reprÃ©sentation linÃ©aire ou matricielle des cartes, classÃ©es par type, raretÃ©, coÃ»t, etc.

ğŸ§  **Avantage** : TrÃ¨s bon pour dÃ©tecter des synergies locales (par exemple, des mini-combos de 2 ou 3 cartes).

[â¬† Retour Ã  la table des matiÃ¨res](#table-des-matiÃ¨res)


</details>

---

### 13. Transformer
<details>
<summary>Voir la dÃ©finition</summary>

Un rÃ©seau basÃ© sur le mÃ©canisme dâ€™attention, permettant dâ€™analyser de longues sÃ©quences en parallÃ¨le sans rÃ©currence.

ğŸ§ª **Application Ã  ton projet** :
- Traiter un deck comme une sÃ©quence de cartes, et apprendre quelles cartes interagissent entre elles via des mÃ©canismes dâ€™attention
- Peut servir Ã  gÃ©nÃ©rer un deck complet token par token, comme GPT gÃ©nÃ¨re du texte mot par mot

ğŸ§  **Avantage** : IdÃ©al pour capturer des relations complexes et non locales entre cartes dans un deck.

[â¬† Retour Ã  la table des matiÃ¨res](#table-des-matiÃ¨res)


</details>

---

### 14. GNN (Graph Neural Network)
<details>
<summary>Voir la dÃ©finition</summary>

Les rÃ©seaux de neurones pour graphes apprennent Ã  partir de structures oÃ¹ les entitÃ©s (nÅ“uds) sont connectÃ©es (arÃªtes), comme un rÃ©seau de synergies.

ğŸ§ª **Application Ã  ton projet** :
- Chaque carte = un nÅ“ud
- Une synergie ou interaction = une arÃªte entre deux cartes
- Le rÃ©seau apprend Ã  Ã©valuer un deck en fonction de la structure de ses synergies

ğŸ§  **Avantage** : Le plus adaptÃ© si tu veux reprÃ©senter la structure interne dâ€™un deck comme un graphe de synergies, de types, ou dâ€™effets complÃ©mentaires.

[â¬† Retour Ã  la table des matiÃ¨res](#table-des-matiÃ¨res)


</details>

---

### 15. AutoML (Automated Machine Learning)
<details>
<summary>Voir la dÃ©finition</summary>

AutoML dÃ©signe l'ensemble des techniques qui permettent d'automatiser tout ou partie du processus de conception d'un modÃ¨le de machine learning, de la prÃ©paration des donnÃ©es jusquâ€™au choix du modÃ¨le et de ses hyperparamÃ¨tres.

ğŸ§ª **Application Ã  ton projet** :
- Tu peux automatiser le choix du meilleur modÃ¨le pour prÃ©dire la force dâ€™un deck ou pour sÃ©lectionner des cartes
- Exemple : essayer automatiquement [MLP](#10-mlp-multi-layer-perceptron), [CNN (Convolutional Neural Network)](#12-cnn-convolutional-neural-network), [LSTM](#11-lstm-long-short-term-memory), etc., sur une tÃ¢che comme "Ã©valuer un deck"

ğŸ§  **Avantage** : Tu gagnes du temps et tu laisses lâ€™outil explorer ce que tu ne soupÃ§onnes mÃªme pas, surtout utile en dÃ©but de projet.

[â¬† Retour Ã  la table des matiÃ¨res](#table-des-matiÃ¨res)

</details>

---

### 16. AutoRL (Automated Reinforcement Learning)
<details>
<summary>Voir la dÃ©finition</summary>

Une branche dâ€™AutoML spÃ©cialisÃ©e dans le RL : elle vise Ã  automatiser le choix de lâ€™algorithme RL, de la politique, du [Reward shaping](#02-reward-shaping), des hyperparamÃ¨tres et de lâ€™architecture rÃ©seau.

ğŸ§ª **Application Ã  ton projet** :
- Ton environnement IA (le jeu PokÃ©mon TCG Pocket) est entiÃ¨rement simulable, donc parfait pour une exploration auto-entretenue
- Exemple : tu veux tester automatiquement [PPO](#06-ppo-proximal-policy-optimization), [DQN](#05-dqn-deep-q-network), [A2C](#07-a2c--a3c-advantage-actor-critic--asynchronous-a2c) avec diffÃ©rentes architectures ([MLP](#10-mlp-multi-layer-perceptron), [LSTM](#11-lstm-long-short-term-memory), [GNN](#14-gnn-graph-neural-network)), et diffÃ©rentes fonctions de rÃ©compense (winrate, diversitÃ© du deck, synergies...)

ğŸ§  **Avantage** : Laisse ton IA apprendre quelle stratÃ©gie de RL est la meilleure pour ton jeu, au lieu de tout faire manuellement.

[â¬† Retour Ã  la table des matiÃ¨res](#table-des-matiÃ¨res)

</details>

---

### 17. NAS (Neural Architecture Search)
<details>
<summary>Voir la dÃ©finition</summary>

Technique dâ€™AutoML qui cherche automatiquement la meilleure architecture de rÃ©seau de neurones pour une tÃ¢che donnÃ©e. Peut Ãªtre combinÃ©e avec RL, Ã©volution gÃ©nÃ©tique, ou recherche bayÃ©sienne.

ğŸ§ª **Application Ã  ton projet** :
- Tu veux quâ€™un moteur NAS teste et dÃ©couvre si une architecture [Transformer](#13-transformer), hybride [CNN](#12-cnn-convolutional-neural-network)+[MLP](#10-mlp-multi-layer-perceptron) ou [GNN](#14-gnn-graph-neural-network) fonctionne le mieux pour prÃ©dire lâ€™efficacitÃ© dâ€™un deck ou pour prendre des dÃ©cisions de construction
- Cela peut aussi tâ€™aider Ã  concevoir le policy network de ton agent RL automatiquement

ğŸ§  **Avantage** : Tu nâ€™as pas besoin de deviner lâ€™architecture idÃ©ale pour ton modÃ¨le de deckbuilder â€” elle est dÃ©couverte automatiquement.

[â¬† Retour Ã  la table des matiÃ¨res](#table-des-matiÃ¨res)

</details>

---

### 18. Ray Tune
<details>
<summary>Voir la dÃ©finition</summary>

Un framework dâ€™optimisation dâ€™hyperparamÃ¨tres Ã  grande Ã©chelle, trÃ¨s utilisÃ© dans les pipelines de RL. IntÃ¨gre facilement des algos de recherche (Bayesian, grid search, PBT, etc.) et peut gÃ©rer des milliers dâ€™expÃ©riences en parallÃ¨le.

ğŸ§ª **Application Ã  ton projet** :
- Tu peux utiliser Ray Tune pour lancer des dizaines de versions de ton agent PPO avec des combinaisons diffÃ©rentes de :
    - learning rate
    - Î³ (discount factor)
    - batch size
    - structure du rÃ©seau (nombre de couches, etc.)

ğŸ§  **Avantage** : Tu peux trouver rapidement les meilleurs rÃ©glages en profitant de la parallÃ©lisation (CPU ou GPU) et de lâ€™intÃ©gration avec des frameworks comme PyTorch, RLlib, etc.

[â¬† Retour Ã  la table des matiÃ¨res](#table-des-matiÃ¨res)

</details>

---

### 19. Optuna / HyperOpt / Nevergrad
<details>
<summary>Voir la dÃ©finition</summary>

Trois outils open-source dâ€™optimisation dâ€™hyperparamÃ¨tres :
- Optuna : recherche bayÃ©sienne et TPE (Tree-structured Parzen Estimator)
- HyperOpt : orientÃ© statistiques bayÃ©siennes Ã©galement
- Nevergrad (by Facebook) : basÃ© sur des stratÃ©gies dâ€™optimisation sans gradient, trÃ¨s utile en contexte RL

ğŸ§ª **Application Ã  ton projet** :
- Tu peux les utiliser pour trouver automatiquement la meilleure configuration dâ€™un agent RL ou dâ€™un modÃ¨le dâ€™Ã©valuation de deck ([MLP](#10-mlp-multi-layer-perceptron) par ex.)
- Exemple : tuning de ton algorithme PPO avec Optuna pour tester des dizaines de combinaisons automatiquement

ğŸ§  **Avantage** : Simple Ã  intÃ©grer dans ton code Python, rapide et efficace pour des petits comme des grands projets.

[â¬† Retour Ã  la table des matiÃ¨res](#table-des-matiÃ¨res)

</details>

---

### 20. OpenAI Gym / Gymnasium
<details>
<summary>Voir la dÃ©finition</summary>

- OpenAI Gym est lâ€™interface standard pour les environnements RL, proposÃ©e par OpenAI.
- Gymnasium est son fork officiel maintenu activement depuis 2022, car Gym nâ€™est plus mis Ã  jour.

ğŸ§ª **Application Ã  ton projet** :
- Tu peux crÃ©er un environnement PokÃ©mon TCG Pocket en suivant le format Gymnasium :
    ```python
    class PokeDeckEnv(gym.Env):
        def step(self, action): ...
        def reset(self): ...
        def render(self): ...
    ```
- Cela te permettra de lâ€™utiliser avec nâ€™importe quelle librairie RL compatible Gym (comme SB3, [Ray RLlib](#24-rllib-ray)...)

ğŸ§  **Avantage** : Standardisation = intÃ©gration facile avec des dizaines dâ€™outils, visualisation simple, outils de debug inclus.

[â¬† Retour Ã  la table des matiÃ¨res](#table-des-matiÃ¨res)

</details>

---

### 21. RLCard
<details>
<summary>Voir la dÃ©finition</summary>

Un framework open-source pour lâ€™entraÃ®nement dâ€™agents RL dans les jeux de cartes (Texas Holdâ€™em, Uno, Blackjack...), dÃ©veloppÃ© par lâ€™universitÃ© de Fudan.

ğŸ§ª **Application Ã  ton projet** :
- RLCard offre des environnements multi-agents avec gestion des rÃ¨gles, du piochement, des actions valides, etc.
- Tu peux tâ€™en inspirer pour structurer ton environnement PokÃ©mon TCG Pocket de faÃ§on modulaire :
    - gestion des Ã©tats de la main
    - deck simulÃ©
    - actions comme "poser carte", "faire Ã©voluer", etc.

ğŸ§  **Avantage** : Tu gagnes du temps en rÃ©utilisant des mÃ©caniques propres aux jeux de cartes, dÃ©jÃ  codÃ©es et testÃ©es.

[â¬† Retour Ã  la table des matiÃ¨res](#table-des-matiÃ¨res)

</details>

---

### 22. PettingZoo
<details>
<summary>Voir la dÃ©finition</summary>

Une librairie Python dÃ©diÃ©e aux environnements RL multi-agents, compatible avec [Gymnasium](#20-openai-gym--gymnasium).

ğŸ§ª **Application Ã  ton projet** :
- Si tu veux simuler deux agents IA qui sâ€™affrontent, PettingZoo est fait pour toi :
    - Chaque joueur est un agent
    - Tu peux faire apprendre les deux agents en parallÃ¨le ou en opposition

ğŸ§  **Avantage** : Permet de former ton IA contre des adversaires dynamiques, au lieu dâ€™un bot statique. Câ€™est idÃ©al pour lâ€™optimisation de decks face Ã  un mÃ©ta-jeu Ã©volutif.

[â¬† Retour Ã  la table des matiÃ¨res](#table-des-matiÃ¨res)

</details>

---

### 23. Stable-Baselines3 (SB3)
<details>
<summary>Voir la dÃ©finition</summary>

Une implÃ©mentation stable, modulaire et bien documentÃ©e des principaux algorithmes de RL ([DQN](#05-dqn-deep-q-network), [PPO](#06-ppo-proximal-policy-optimization), [A2C / A3C](#07-a2c--a3c-advantage-actor-critic--asynchronous-a2c)...) basÃ©e sur PyTorch.

ğŸ§ª **Application Ã  ton projet** :
- Une fois ton environnement Gym prÃªt, tu peux directement lâ€™entraÃ®ner avec SB3 :
```python
from stable_baselines3 import PPO
model = PPO("MlpPolicy", poke_env, verbose=1)
model.learn(total_timesteps=100_000)
```

ğŸ§  **Avantage** : Rapide Ã  dÃ©ployer, excellente documentation, plugins pour TensorBoard, checkpoints, callbacks personnalisÃ©s...

[â¬† Retour Ã  la table des matiÃ¨res](#table-des-matiÃ¨res)

</details>

---

### 24. RLlib (Ray)
<details>
<summary>Voir la dÃ©finition</summary>

Une plateforme RL industrielle et distribuÃ©e, intÃ©grÃ©e Ã  lâ€™Ã©cosystÃ¨me Ray. Permet de lancer des expÃ©riences RL Ã  grande Ã©chelle, avec support pour la parallÃ©lisation, les multi-agents, et lâ€™AutoRL.

ğŸ§ª **Application Ã  ton projet** :
- IdÃ©al si tu veux entraÃ®ner plusieurs agents deckbuilders en parallÃ¨le, ou lancer des recherches dâ€™hyperparamÃ¨tres avec [Ray Tune](#18-ray-tune)
- Tu peux lâ€™utiliser pour du training multi-joueur oÃ¹ chaque IA explore des stratÃ©gies diffÃ©rentes

ğŸ§  **Avantage** : Puissant, scalable, prÃªt pour le cloud. Câ€™est lâ€™outil parfait pour passer dâ€™un prototype local Ã  un entraÃ®nement massif.

[â¬† Retour Ã  la table des matiÃ¨res](#table-des-matiÃ¨res)

</details>

---
